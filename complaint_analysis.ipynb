import pandas as pd
import re
import json
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import chi2

nltk.download(['punkt', 'stopwords', 'wordnet'])

class ComplaintLexiconAnalyzer:
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
        self.lexicon = pd.DataFrame()
        self.regex_rules = {}

    def _preprocess_text(self, text):
        """Clean and normalize text content"""
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
        text = re.sub(r'\d+', '', text)       # Remove numbers
        tokens = nltk.word_tokenize(text)
        tokens = [self.lemmatizer.lemmatize(word) for word in tokens
                  if word not in self.stop_words and len(word) > 2]
        return ' '.join(tokens)

    def generate_lexicon(self, emails, labels, top_terms=100):
        """
        Generate complaint lexicon from historical data
        :param emails: List of email texts
        :param labels: List of binary labels (1=complaint, 0=non-complaint)
        :param top_terms: Number of terms to include in lexicon
        """
        # Preprocess texts
        preprocessed = [self._preprocess_text(email) for email in emails]
        
        # Vectorize text using TF-IDF
        vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=2000)
        X = vectorizer.fit_transform(preprocessed)
        
        # Calculate chi-squared scores
        chi2_scores, _ = chi2(X, labels)
        
        # Create lexicon dataframe
        self.lexicon = pd.DataFrame({
            'term': vectorizer.get_feature_names_out(),
            'chi2_score': chi2_scores,
            'occurrence_count': X.sum(axis=0).A1,
            'category': 'uncategorized',
            'regex_pattern': '',
            'sample_usage': ''
        }).sort_values('chi2_score', ascending=False).head(top_terms)

        # Generate regex patterns and sample matches
        self._add_regex_patterns()
        self._add_sample_usages(emails)
        
        return self.lexicon

    def _add_regex_patterns(self):
        """Generate regex patterns for lexicon terms"""
        patterns = []
        for term in self.lexicon['term']:
            # Handle multi-word terms with word boundaries
            words = term.split()
            pattern = r'\b' + r'\s+'.join([r'\w*' + re.escape(word) + r'\w*' 
                      for word in words]) + r'\b'
            patterns.append(f'(?i){pattern}')  # Case-insensitive
        self.lexicon['regex_pattern'] = patterns

    def _add_sample_usages(self, emails):
        """Find example usages of terms in original emails"""
        samples = []
        for term in self.lexicon['term']:
            matches = []
            for email in emails:
                if re.search(r'\b' + term + r'\b', email, re.I):
                    matches.append(email[:150] + '...')
                    if len(matches) >= 3:
                        break
            samples.append('\n'.join(matches))
        self.lexicon['sample_usage'] = samples

    def save_lexicon(self, filename='complaint_lexicon.csv'):
        """Save lexicon dataset to file"""
        self.lexicon.to_csv(filename, index=False)
        print(f"Lexicon dataset saved to {filename}")

    def generate_regex_rules(self, output_file='regex_rules.json'):
        """Generate JSON file with regex rules from lexicon"""
        rules = {
            'complaint_terms': '|'.join(self.lexicon['regex_pattern'].tolist()),
            'negation_patterns': [
                r'\b(not|never|no|n\'t|don\'t|without)\b\s+\w+',
                r'\b(no\s+longer|at\s+all|nothing|none)\b'
            ],
            'intensifiers': r'\b(extremely|urgently|critical|unacceptable|outrageous)\b'
        }
        
        with open(output_file, 'w') as f:
            json.dump(rules, f, indent=2)
        
        print(f"Regex rules saved to {output_file}")

class ComplaintDetector:
    def __init__(self, lexicon_path='complaint_lexicon.csv'):
        self.lexicon = pd.read_csv(lexicon_path)
        self.patterns = self._load_regex_patterns()
    
    def _load_regex_patterns(self):
        """Compile regex patterns from lexicon"""
        patterns = {
            'terms': re.compile(
                '(' + ')|('.join(self.lexicon['regex_pattern'].tolist()) + ')',
                re.IGNORECASE
            ),
            'negations': re.compile(
                r'\b(not|never|no|n\'t|don\'t|without)\b\s+\w+',
                re.IGNORECASE
            ),
            'intensifiers': re.compile(
                r'\b(extremely|urgently|critical|unacceptable)\b',
                re.IGNORECASE
            )
        }
        return patterns
    
    def analyze_email(self, email, threshold=3):
        """Analyze email content for potential complaints"""
        results = {
            'score': 0,
            'matched_terms': [],
            'negations_found': [],
            'intensifiers_found': [],
            'is_complaint': False
        }
        
        # Match complaint terms
        term_matches = self.patterns['terms'].findall(email)
        results['matched_terms'] = [m[0] for m in term_matches if m[0]]
        results['score'] += len(results['matched_terms'])
        
        # Match negations
        results['negations_found'] = self.patterns['negations'].findall(email)
        results['score'] += len(results['negations_found'])
        
        # Match intensifiers
        results['intensifiers_found'] = self.patterns['intensifiers'].findall(email)
        results['score'] += len(results['intensifiers_found'])
        
        # Final determination
        results['is_complaint'] = results['score'] >= threshold
        
        return results

# Example Usage
if __name__ == "__main__":
    # Sample training data
    train_emails = [
        "Terrible service! My order never arrived and you won't respond to my emails",
        "Very happy with the quick delivery and quality product",
        "The product broke after 2 days of use, need immediate replacement",
        "Please cancel my subscription, this service is useless",
        "Excellent customer support experience",
        "I'm extremely disappointed with the defective product I received"
    ]
    train_labels = [1, 0, 1, 1, 0, 1]

    # 1. Generate and save lexicon
    analyzer = ComplaintLexiconAnalyzer()
    lexicon = analyzer.generate_lexicon(train_emails, train_labels, top_terms=15)
    analyzer.save_lexicon()
    analyzer.generate_regex_rules()

    # 2. Use the detector
    detector = ComplaintDetector()
    
    test_email = """Dear Support,
    I'm writing to express my extreme dissatisfaction with your service. 
    The product I received was completely broken and your customer support 
    team has not responded to my repeated requests for a refund. This is 
    completely unacceptable and I will never purchase from your company again."""
    
    analysis = detector.analyze_email(test_email)
    
    print("\nEmail Analysis Results:")
    print(f"Complaint Detected: {analysis['is_complaint']}")
    print(f"Total Score: {analysis['score']}")
    print("Matched Terms:", analysis['matched_terms'])
    print("Negations Found:", analysis['negations_found'])
    print("Intensifiers Found:", analysis['intensifiers_found'])

    # Show lexicon dataset
    print("\nSample Lexicon Dataset:")
    print(lexicon[['term', 'chi2_score', 'category', 'regex_pattern']].head())
