Slide 1: Title Slide
Title: Lexicon-Based Complaint Identification: Initial Results & Validation Pathway
Subtitle: 6-Week Data Analysis for Research Team Review
Team Name | Date

Slide 2: Agenda
Lexicon Strategy Overview

6-Week Data Insights

Validation Process with Research Team

Next Steps & Timeline

Slide 3: Lexicon Strategy Overview
Objective:

Automatically flag potential complaint emails using predefined keywords/sentiment rules (e.g., “refund,” “broken,” “unacceptable”).

Generate a dataset for research team validation against system-logged complaints.

Methodology:

Lexicon Development: Curated list of 100+ complaint-related keywords/phrases.

Rule-Based Filtering: Emails tagged if they contain ≥1 high-priority keyword or ≥2 medium-priority keywords.

Data Export: Flagged emails compiled for research team analysis.

Why Lexicon First?

Simple, transparent baseline for future AI/ML models.

Easily adjustable rules based on feedback.

Slide 4: 6-Week Data Snapshot
Slide 4: 6-Week Data Snapshot
Objective: Share raw results of lexicon-based flagging for research team validation.

Key Metrics:

Total Inbound Emails: [X] (e.g., 5,000)

Flagged as Potential Complaints: [Y] (e.g., 750 | 15% of total)

Breakdown of Flagged Emails by Category:

Billing Issues: [A%]

Product Defects: [B%]

Service Delays: [C%]

Other: [D%]

Visual:

Bar Chart 1: Total emails vs. flagged complaints.

Pie Chart: Distribution of complaint categories.

Key Observations:

Lexon rules flagged 15% of total emails as potential complaints.

Top category: Billing Issues (40% of flagged emails).

Slide 5: Next Steps – Research Team Validation & Feedback
Objective: Validate flagged emails against system-logged complaints and refine lexicon rules.

Process:

Data Handoff to Research Team:

Provide 6 weeks of flagged emails (750) with metadata (date, sender, keywords).

Share system complaint logs for the same period.

Validation Tasks for Research Team:

Step 1: Compare flagged emails with system-logged complaints.

Are flagged emails actually complaints?

Are system-logged complaints missed by the lexicon?

Step 2: Categorize discrepancies:

False Positives: Flagged emails that are not complaints (e.g., “problem” used neutrally).

False Negatives: Complaints in the system not flagged by the lexicon.

Feedback Sessions:

Joint workshops to review findings.

Refine lexicon rules to:

Reduce False Positives: Remove/contextualize ambiguous keywords.

Improve Coverage: Add missing complaint phrases (e.g., “escalate,” “compensation”).

Expected Outcomes:

Sharper Lexicon Rules: Reduce alert volume by [X%].

Actionable Insights: Prioritize high-impact complaint categories (e.g., billing).

Timeline:

Week 1: Data handoff and alignment.

Week 2-3: Research team analysis.

Week 4: Feedback and rule refinement.

Visual:

Process Flow Diagram: Data handoff → validation → feedback → refinement.

Example Snippets: Show 1–2 anonymized false positive/negative emails.

Key Adjustments Based on Your Feedback:
Slide 4 now purely focuses on volumes and categories (no precision/recall metrics).

Slide 5 emphasizes the collaboration with the research team, framing validation as the critical next step to refine rules and reduce noise.

Added concrete examples of how false positives/negatives will be addressed.

Let me know if you’d like to tweak the visuals or expand on specific sections!
